{"version":3,"file":"index.mjs","sources":["../../../../src/llm/bedrock/index.ts"],"sourcesContent":["/**\n * Optimized ChatBedrockConverse wrapper that fixes contentBlockIndex conflicts\n * and adds support for latest @langchain/aws features:\n *\n * - Application Inference Profiles (PR #9129)\n * - Service Tiers (Priority/Standard/Flex) (PR #9785) - requires AWS SDK 3.966.0+\n *\n * Bedrock sends the same contentBlockIndex for both text and tool_use content blocks,\n * causing LangChain's merge logic to fail with \"field[contentBlockIndex] already exists\"\n * errors. This wrapper simply strips contentBlockIndex from response_metadata to avoid\n * the conflict.\n *\n * The contentBlockIndex field is only used internally by Bedrock's streaming protocol\n * and isn't needed by application logic - the index field on tool_call_chunks serves\n * the purpose of tracking tool call ordering.\n */\n\nimport { ChatBedrockConverse } from '@langchain/aws';\nimport { AIMessageChunk } from '@langchain/core/messages';\nimport { ChatGenerationChunk, ChatResult } from '@langchain/core/outputs';\nimport type { CallbackManagerForLLMRun } from '@langchain/core/callbacks/manager';\nimport type { ChatBedrockConverseInput } from '@langchain/aws';\nimport type { BaseMessage } from '@langchain/core/messages';\n\n/**\n * Service tier type for Bedrock invocations.\n * Requires AWS SDK >= 3.966.0 to actually work.\n * @see https://docs.aws.amazon.com/bedrock/latest/userguide/service-tiers-inference.html\n */\nexport type ServiceTierType = 'priority' | 'default' | 'flex' | 'reserved';\n\n/**\n * Extended input interface with additional features:\n * - applicationInferenceProfile: Use an inference profile ARN instead of model ID\n * - serviceTier: Specify service tier (Priority, Standard, Flex, Reserved)\n */\nexport interface CustomChatBedrockConverseInput\n  extends ChatBedrockConverseInput {\n  /**\n   * Application Inference Profile ARN to use for the model.\n   * For example, \"arn:aws:bedrock:eu-west-1:123456789102:application-inference-profile/fm16bt65tzgx\"\n   * When provided, this ARN will be used for the actual inference calls instead of the model ID.\n   * Must still provide `model` as normal modelId to benefit from all the metadata.\n   * @see https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-create.html\n   */\n  applicationInferenceProfile?: string;\n\n  /**\n   * Service tier for model invocation.\n   * Specifies the processing tier type used for serving the request.\n   * Supported values are 'priority', 'default', 'flex', and 'reserved'.\n   *\n   * - 'priority': Prioritized processing for lower latency\n   * - 'default': Standard processing tier\n   * - 'flex': Flexible processing tier with lower cost\n   * - 'reserved': Reserved capacity for consistent performance\n   *\n   * If not provided, AWS uses the default tier.\n   * Note: Requires AWS SDK >= 3.966.0 to work.\n   * @see https://docs.aws.amazon.com/bedrock/latest/userguide/service-tiers-inference.html\n   */\n  serviceTier?: ServiceTierType;\n}\n\n/**\n * Extended call options with serviceTier override support.\n */\nexport interface CustomChatBedrockConverseCallOptions {\n  serviceTier?: ServiceTierType;\n}\n\nexport class CustomChatBedrockConverse extends ChatBedrockConverse {\n  /**\n   * Application Inference Profile ARN to use instead of model ID.\n   */\n  applicationInferenceProfile?: string;\n\n  /**\n   * Service tier for model invocation.\n   */\n  serviceTier?: ServiceTierType;\n\n  constructor(fields?: CustomChatBedrockConverseInput) {\n    super(fields);\n    this.applicationInferenceProfile = fields?.applicationInferenceProfile;\n    this.serviceTier = fields?.serviceTier;\n  }\n\n  static lc_name(): string {\n    return 'LibreChatBedrockConverse';\n  }\n\n  /**\n   * Get the model ID to use for API calls.\n   * Returns applicationInferenceProfile if set, otherwise returns this.model.\n   */\n  protected getModelId(): string {\n    return this.applicationInferenceProfile ?? this.model;\n  }\n\n  /**\n   * Override invocationParams to add serviceTier support.\n   */\n  override invocationParams(\n    options?: this['ParsedCallOptions'] & CustomChatBedrockConverseCallOptions\n  ): ReturnType<ChatBedrockConverse['invocationParams']> & {\n    serviceTier?: { type: ServiceTierType };\n  } {\n    const baseParams = super.invocationParams(options);\n\n    /** Service tier from options or fall back to class-level setting */\n    const serviceTierType = options?.serviceTier ?? this.serviceTier;\n\n    return {\n      ...baseParams,\n      serviceTier: serviceTierType ? { type: serviceTierType } : undefined,\n    };\n  }\n\n  /**\n   * Override _generateNonStreaming to use applicationInferenceProfile as modelId.\n   * Uses the same model-swapping pattern as streaming for consistency.\n   */\n  override async _generateNonStreaming(\n    messages: BaseMessage[],\n    options: this['ParsedCallOptions'] & CustomChatBedrockConverseCallOptions,\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    // Temporarily swap model for applicationInferenceProfile support\n    const originalModel = this.model;\n    if (\n      this.applicationInferenceProfile != null &&\n      this.applicationInferenceProfile !== ''\n    ) {\n      this.model = this.applicationInferenceProfile;\n    }\n\n    try {\n      return await super._generateNonStreaming(messages, options, runManager);\n    } finally {\n      // Restore original model\n      this.model = originalModel;\n    }\n  }\n\n  /**\n   * Override _streamResponseChunks to:\n   * 1. Use applicationInferenceProfile as modelId (by temporarily swapping this.model)\n   * 2. Strip contentBlockIndex from response_metadata to prevent merge conflicts\n   *\n   * Note: We delegate to super._streamResponseChunks() to preserve @langchain/aws's\n   * internal chunk handling which correctly preserves array content for reasoning blocks.\n   */\n  override async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this['ParsedCallOptions'] & CustomChatBedrockConverseCallOptions,\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    // Temporarily swap model for applicationInferenceProfile support\n    const originalModel = this.model;\n    if (\n      this.applicationInferenceProfile != null &&\n      this.applicationInferenceProfile !== ''\n    ) {\n      this.model = this.applicationInferenceProfile;\n    }\n\n    try {\n      // Use parent's streaming logic which correctly handles reasoning content\n      const baseStream = super._streamResponseChunks(\n        messages,\n        options,\n        runManager\n      );\n\n      for await (const chunk of baseStream) {\n        // Clean contentBlockIndex from response_metadata to prevent merge conflicts\n        yield this.cleanChunk(chunk);\n      }\n    } finally {\n      // Restore original model\n      this.model = originalModel;\n    }\n  }\n\n  /**\n   * Clean a chunk by removing contentBlockIndex from response_metadata.\n   */\n  private cleanChunk(chunk: ChatGenerationChunk): ChatGenerationChunk {\n    const message = chunk.message;\n    if (!(message instanceof AIMessageChunk)) {\n      return chunk;\n    }\n\n    const metadata = message.response_metadata as Record<string, unknown>;\n    const hasContentBlockIndex = this.hasContentBlockIndex(metadata);\n    if (!hasContentBlockIndex) {\n      return chunk;\n    }\n\n    const cleanedMetadata = this.removeContentBlockIndex(metadata) as Record<\n      string,\n      unknown\n    >;\n\n    return new ChatGenerationChunk({\n      text: chunk.text,\n      message: new AIMessageChunk({\n        ...message,\n        response_metadata: cleanedMetadata,\n      }),\n      generationInfo: chunk.generationInfo,\n    });\n  }\n\n  /**\n   * Check if contentBlockIndex exists at any level in the object\n   */\n  private hasContentBlockIndex(obj: unknown): boolean {\n    if (obj === null || obj === undefined || typeof obj !== 'object') {\n      return false;\n    }\n\n    if ('contentBlockIndex' in obj) {\n      return true;\n    }\n\n    for (const value of Object.values(obj)) {\n      if (typeof value === 'object' && value !== null) {\n        if (this.hasContentBlockIndex(value)) {\n          return true;\n        }\n      }\n    }\n\n    return false;\n  }\n\n  /**\n   * Recursively remove contentBlockIndex from all levels of an object\n   */\n  private removeContentBlockIndex(obj: unknown): unknown {\n    if (obj === null || obj === undefined) {\n      return obj;\n    }\n\n    if (Array.isArray(obj)) {\n      return obj.map((item) => this.removeContentBlockIndex(item));\n    }\n\n    if (typeof obj === 'object') {\n      const cleaned: Record<string, unknown> = {};\n      for (const [key, value] of Object.entries(obj)) {\n        if (key !== 'contentBlockIndex') {\n          cleaned[key] = this.removeContentBlockIndex(value);\n        }\n      }\n      return cleaned;\n    }\n\n    return obj;\n  }\n}\n\nexport type { ChatBedrockConverseInput };\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;;;;;;;;AAeG;AAwDG,MAAO,yBAA0B,SAAQ,mBAAmB,CAAA;AAChE;;AAEG;AACH,IAAA,2BAA2B;AAE3B;;AAEG;AACH,IAAA,WAAW;AAEX,IAAA,WAAA,CAAY,MAAuC,EAAA;QACjD,KAAK,CAAC,MAAM,CAAC;AACb,QAAA,IAAI,CAAC,2BAA2B,GAAG,MAAM,EAAE,2BAA2B;AACtE,QAAA,IAAI,CAAC,WAAW,GAAG,MAAM,EAAE,WAAW;;AAGxC,IAAA,OAAO,OAAO,GAAA;AACZ,QAAA,OAAO,0BAA0B;;AAGnC;;;AAGG;IACO,UAAU,GAAA;AAClB,QAAA,OAAO,IAAI,CAAC,2BAA2B,IAAI,IAAI,CAAC,KAAK;;AAGvD;;AAEG;AACM,IAAA,gBAAgB,CACvB,OAA0E,EAAA;QAI1E,MAAM,UAAU,GAAG,KAAK,CAAC,gBAAgB,CAAC,OAAO,CAAC;;QAGlD,MAAM,eAAe,GAAG,OAAO,EAAE,WAAW,IAAI,IAAI,CAAC,WAAW;QAEhE,OAAO;AACL,YAAA,GAAG,UAAU;AACb,YAAA,WAAW,EAAE,eAAe,GAAG,EAAE,IAAI,EAAE,eAAe,EAAE,GAAG,SAAS;SACrE;;AAGH;;;AAGG;AACM,IAAA,MAAM,qBAAqB,CAClC,QAAuB,EACvB,OAAyE,EACzE,UAAqC,EAAA;;AAGrC,QAAA,MAAM,aAAa,GAAG,IAAI,CAAC,KAAK;AAChC,QAAA,IACE,IAAI,CAAC,2BAA2B,IAAI,IAAI;AACxC,YAAA,IAAI,CAAC,2BAA2B,KAAK,EAAE,EACvC;AACA,YAAA,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,2BAA2B;;AAG/C,QAAA,IAAI;YACF,OAAO,MAAM,KAAK,CAAC,qBAAqB,CAAC,QAAQ,EAAE,OAAO,EAAE,UAAU,CAAC;;gBAC/D;;AAER,YAAA,IAAI,CAAC,KAAK,GAAG,aAAa;;;AAI9B;;;;;;;AAOG;IACM,OAAO,qBAAqB,CACnC,QAAuB,EACvB,OAAyE,EACzE,UAAqC,EAAA;;AAGrC,QAAA,MAAM,aAAa,GAAG,IAAI,CAAC,KAAK;AAChC,QAAA,IACE,IAAI,CAAC,2BAA2B,IAAI,IAAI;AACxC,YAAA,IAAI,CAAC,2BAA2B,KAAK,EAAE,EACvC;AACA,YAAA,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,2BAA2B;;AAG/C,QAAA,IAAI;;AAEF,YAAA,MAAM,UAAU,GAAG,KAAK,CAAC,qBAAqB,CAC5C,QAAQ,EACR,OAAO,EACP,UAAU,CACX;AAED,YAAA,WAAW,MAAM,KAAK,IAAI,UAAU,EAAE;;AAEpC,gBAAA,MAAM,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC;;;gBAEtB;;AAER,YAAA,IAAI,CAAC,KAAK,GAAG,aAAa;;;AAI9B;;AAEG;AACK,IAAA,UAAU,CAAC,KAA0B,EAAA;AAC3C,QAAA,MAAM,OAAO,GAAG,KAAK,CAAC,OAAO;AAC7B,QAAA,IAAI,EAAE,OAAO,YAAY,cAAc,CAAC,EAAE;AACxC,YAAA,OAAO,KAAK;;AAGd,QAAA,MAAM,QAAQ,GAAG,OAAO,CAAC,iBAA4C;QACrE,MAAM,oBAAoB,GAAG,IAAI,CAAC,oBAAoB,CAAC,QAAQ,CAAC;QAChE,IAAI,CAAC,oBAAoB,EAAE;AACzB,YAAA,OAAO,KAAK;;QAGd,MAAM,eAAe,GAAG,IAAI,CAAC,uBAAuB,CAAC,QAAQ,CAG5D;QAED,OAAO,IAAI,mBAAmB,CAAC;YAC7B,IAAI,EAAE,KAAK,CAAC,IAAI;YAChB,OAAO,EAAE,IAAI,cAAc,CAAC;AAC1B,gBAAA,GAAG,OAAO;AACV,gBAAA,iBAAiB,EAAE,eAAe;aACnC,CAAC;YACF,cAAc,EAAE,KAAK,CAAC,cAAc;AACrC,SAAA,CAAC;;AAGJ;;AAEG;AACK,IAAA,oBAAoB,CAAC,GAAY,EAAA;AACvC,QAAA,IAAI,GAAG,KAAK,IAAI,IAAI,GAAG,KAAK,SAAS,IAAI,OAAO,GAAG,KAAK,QAAQ,EAAE;AAChE,YAAA,OAAO,KAAK;;AAGd,QAAA,IAAI,mBAAmB,IAAI,GAAG,EAAE;AAC9B,YAAA,OAAO,IAAI;;QAGb,KAAK,MAAM,KAAK,IAAI,MAAM,CAAC,MAAM,CAAC,GAAG,CAAC,EAAE;YACtC,IAAI,OAAO,KAAK,KAAK,QAAQ,IAAI,KAAK,KAAK,IAAI,EAAE;AAC/C,gBAAA,IAAI,IAAI,CAAC,oBAAoB,CAAC,KAAK,CAAC,EAAE;AACpC,oBAAA,OAAO,IAAI;;;;AAKjB,QAAA,OAAO,KAAK;;AAGd;;AAEG;AACK,IAAA,uBAAuB,CAAC,GAAY,EAAA;QAC1C,IAAI,GAAG,KAAK,IAAI,IAAI,GAAG,KAAK,SAAS,EAAE;AACrC,YAAA,OAAO,GAAG;;AAGZ,QAAA,IAAI,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE;AACtB,YAAA,OAAO,GAAG,CAAC,GAAG,CAAC,CAAC,IAAI,KAAK,IAAI,CAAC,uBAAuB,CAAC,IAAI,CAAC,CAAC;;AAG9D,QAAA,IAAI,OAAO,GAAG,KAAK,QAAQ,EAAE;YAC3B,MAAM,OAAO,GAA4B,EAAE;AAC3C,YAAA,KAAK,MAAM,CAAC,GAAG,EAAE,KAAK,CAAC,IAAI,MAAM,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE;AAC9C,gBAAA,IAAI,GAAG,KAAK,mBAAmB,EAAE;oBAC/B,OAAO,CAAC,GAAG,CAAC,GAAG,IAAI,CAAC,uBAAuB,CAAC,KAAK,CAAC;;;AAGtD,YAAA,OAAO,OAAO;;AAGhB,QAAA,OAAO,GAAG;;AAEb;;;;"}